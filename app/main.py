from fastapi import FastAPI, BackgroundTasks
from contextlib import asynccontextmanager
import os
from dotenv import load_dotenv

from app.crawler.naver_crawler import NaverFinanceCrawler
from app.crawler.hankyung_crawler import HankyungCrawler
from app.crawler.mk_crawler import MKCrawler
from app.crawler.edaily_crawler import EdailyCrawler
from app.crawler.herald_crawler import HeraldCrawler
from app.crawler.yahoo_crawler import YahooCrawler
from app.crawler.reuters_crawler import ReutersCrawler
from app.services.news_service import NewsService
from app.utils.logger import log

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

MAX_NEWS = int(os.getenv("MAX_NEWS_PER_SOURCE", "50"))


# ì•± ë¼ì´í”„ì‚¬ì´í´ ê´€ë¦¬
@asynccontextmanager
async def lifespan(app: FastAPI):
    """ì•± ì‹œì‘/ì¢…ë£Œ ì‹œ ì‹¤í–‰ë˜ëŠ” ì½”ë“œ"""
    log.info("ğŸš€ Lucr Crawler ì‹œì‘")
    yield
    log.info("ğŸ›‘ Lucr Crawler ì¢…ë£Œ")


# FastAPI ì•± ìƒì„±
app = FastAPI(
    title="Lucr News Crawler",
    description="ê¸ˆìœµ ë‰´ìŠ¤ ìë™ ìˆ˜ì§‘ ì‹œìŠ¤í…œ",
    version="1.0.0",
    lifespan=lifespan
)


@app.get("/")
async def root():
    """í—¬ìŠ¤ ì²´í¬"""
    return {
        "service": "Lucr News Crawler",
        "status": "running",
        "version": "1.0.0"
    }


@app.post("/crawl/naver")
async def crawl_naver_news(background_tasks: BackgroundTasks):
    """
    ë„¤ì´ë²„ ê¸ˆìœµ ë‰´ìŠ¤ í¬ë¡¤ë§ (ìˆ˜ë™ ì‹¤í–‰)
    
    Returns:
        í¬ë¡¤ë§ ì‹œì‘ ë©”ì‹œì§€
    """
    log.info("ë„¤ì´ë²„ ê¸ˆìœµ ë‰´ìŠ¤ í¬ë¡¤ë§ ìš”ì²­ ë°›ìŒ")
    
    # ë°±ê·¸ë¼ìš´ë“œì—ì„œ í¬ë¡¤ë§ ì‹¤í–‰
    background_tasks.add_task(run_naver_crawler)
    
    return {
        "message": "ë„¤ì´ë²„ ê¸ˆìœµ ë‰´ìŠ¤ í¬ë¡¤ë§ì´ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤.",
        "status": "started"
    }


@app.post("/crawl/hankyung")
async def crawl_hankyung_news(background_tasks: BackgroundTasks):
    """í•œêµ­ê²½ì œ ë‰´ìŠ¤ í¬ë¡¤ë§"""
    log.info("í•œêµ­ê²½ì œ ë‰´ìŠ¤ í¬ë¡¤ë§ ìš”ì²­ ë°›ìŒ")
    background_tasks.add_task(run_hankyung_crawler)
    return {"message": "í•œêµ­ê²½ì œ ë‰´ìŠ¤ í¬ë¡¤ë§ì´ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤.", "status": "started"}


@app.post("/crawl/mk")
async def crawl_mk_news(background_tasks: BackgroundTasks):
    """ë§¤ì¼ê²½ì œ ë‰´ìŠ¤ í¬ë¡¤ë§"""
    log.info("ë§¤ì¼ê²½ì œ ë‰´ìŠ¤ í¬ë¡¤ë§ ìš”ì²­ ë°›ìŒ")
    background_tasks.add_task(run_mk_crawler)
    return {"message": "ë§¤ì¼ê²½ì œ ë‰´ìŠ¤ í¬ë¡¤ë§ì´ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤.", "status": "started"}


@app.post("/crawl/edaily")
async def crawl_edaily_news(background_tasks: BackgroundTasks):
    """ì´ë°ì¼ë¦¬ ë‰´ìŠ¤ í¬ë¡¤ë§"""
    log.info("ì´ë°ì¼ë¦¬ ë‰´ìŠ¤ í¬ë¡¤ë§ ìš”ì²­ ë°›ìŒ")
    background_tasks.add_task(run_edaily_crawler)
    return {"message": "ì´ë°ì¼ë¦¬ ë‰´ìŠ¤ í¬ë¡¤ë§ì´ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤.", "status": "started"}


@app.post("/crawl/herald")
async def crawl_herald_news(background_tasks: BackgroundTasks):
    """í—¤ëŸ´ë“œê²½ì œ ë‰´ìŠ¤ í¬ë¡¤ë§"""
    log.info("í—¤ëŸ´ë“œê²½ì œ ë‰´ìŠ¤ í¬ë¡¤ë§ ìš”ì²­ ë°›ìŒ")
    background_tasks.add_task(run_herald_crawler)
    return {"message": "í—¤ëŸ´ë“œê²½ì œ ë‰´ìŠ¤ í¬ë¡¤ë§ì´ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤.", "status": "started"}


@app.post("/crawl/yahoo")
async def crawl_yahoo_news(background_tasks: BackgroundTasks):
    """Yahoo Finance ë‰´ìŠ¤ í¬ë¡¤ë§"""
    log.info("Yahoo Finance ë‰´ìŠ¤ í¬ë¡¤ë§ ìš”ì²­ ë°›ìŒ")
    background_tasks.add_task(run_yahoo_crawler)
    return {"message": "Yahoo Finance ë‰´ìŠ¤ í¬ë¡¤ë§ì´ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤.", "status": "started"}


@app.post("/crawl/reuters")
async def crawl_reuters_news(background_tasks: BackgroundTasks):
    """Reuters ë‰´ìŠ¤ í¬ë¡¤ë§"""
    log.info("Reuters ë‰´ìŠ¤ í¬ë¡¤ë§ ìš”ì²­ ë°›ìŒ")
    background_tasks.add_task(run_reuters_crawler)
    return {"message": "Reuters ë‰´ìŠ¤ í¬ë¡¤ë§ì´ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤.", "status": "started"}


@app.post("/crawl/all")
async def crawl_all_news(background_tasks: BackgroundTasks):
    """
    ëª¨ë“  ì¶œì²˜ì˜ ë‰´ìŠ¤ í¬ë¡¤ë§
    
    Returns:
        í¬ë¡¤ë§ ì‹œì‘ ë©”ì‹œì§€
    """
    log.info("ì „ì²´ ë‰´ìŠ¤ í¬ë¡¤ë§ ìš”ì²­ ë°›ìŒ")
    
    # ë°±ê·¸ë¼ìš´ë“œì—ì„œ í¬ë¡¤ë§ ì‹¤í–‰
    background_tasks.add_task(run_all_crawlers)
    
    return {
        "message": "ì „ì²´ ë‰´ìŠ¤ í¬ë¡¤ë§ì´ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤ (7ê°œ ì–¸ë¡ ì‚¬).",
        "status": "started"
    }


async def run_naver_crawler():
    """ë„¤ì´ë²„ í¬ë¡¤ëŸ¬ ì‹¤í–‰ ë° Spring APIë¡œ ì „ì†¡"""
    crawler = NaverFinanceCrawler()
    news_service = NewsService()
    
    try:
        # 1. ë‰´ìŠ¤ í¬ë¡¤ë§
        log.info(f"ë„¤ì´ë²„ ê¸ˆìœµ ë‰´ìŠ¤ í¬ë¡¤ë§ ì‹œì‘ (ìµœëŒ€ {MAX_NEWS}ê°œ)")
        news_list = await crawler.crawl(max_news=MAX_NEWS)
        
        if not news_list:
            log.warning("í¬ë¡¤ë§ëœ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        # 2. Spring APIë¡œ ì „ì†¡
        success_count = 0
        duplicate_count = 0
        error_count = 0
        
        for news in news_list:
            try:
                # URL ì¤‘ë³µ í™•ì¸
                if await news_service.check_url_exists(news.url):
                    log.info(f"ì´ë¯¸ ì¡´ì¬í•˜ëŠ” ë‰´ìŠ¤ (ìŠ¤í‚µ): {news.title[:30]}...")
                    duplicate_count += 1
                    continue
                
                # Spring APIë¡œ ë‰´ìŠ¤ ìƒì„± ìš”ì²­
                news_dto = news.to_create_dto()
                result = await news_service.create_news(news_dto)
                
                if result:
                    success_count += 1
                    log.info(f"ë‰´ìŠ¤ ì €ì¥ ì„±ê³µ [{success_count}]: {news.title[:30]}...")
                else:
                    error_count += 1
                    
            except Exception as e:
                error_count += 1
                log.error(f"ë‰´ìŠ¤ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                continue
        
        # 3. ê²°ê³¼ ë¡œê¹…
        log.info(f"""
        âœ… ë„¤ì´ë²„ ê¸ˆìœµ ë‰´ìŠ¤ í¬ë¡¤ë§ ì™„ë£Œ
        ğŸ“Š í¬ë¡¤ë§: {len(news_list)}ê°œ
        âœ¨ ì €ì¥ ì„±ê³µ: {success_count}ê°œ
        ğŸ”„ ì¤‘ë³µ ìŠ¤í‚µ: {duplicate_count}ê°œ
        âŒ ì˜¤ë¥˜: {error_count}ê°œ
        """)
        
    except Exception as e:
        log.error(f"í¬ë¡¤ëŸ¬ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    finally:
        await news_service.close()


async def run_hankyung_crawler():
    """í•œêµ­ê²½ì œ í¬ë¡¤ëŸ¬ ì‹¤í–‰"""
    crawler = HankyungCrawler()
    await _run_crawler(crawler, "í•œêµ­ê²½ì œ")


async def run_mk_crawler():
    """ë§¤ì¼ê²½ì œ í¬ë¡¤ëŸ¬ ì‹¤í–‰"""
    crawler = MKCrawler()
    await _run_crawler(crawler, "ë§¤ì¼ê²½ì œ")


async def run_edaily_crawler():
    """ì´ë°ì¼ë¦¬ í¬ë¡¤ëŸ¬ ì‹¤í–‰"""
    crawler = EdailyCrawler()
    await _run_crawler(crawler, "ì´ë°ì¼ë¦¬")


async def run_herald_crawler():
    """í—¤ëŸ´ë“œê²½ì œ í¬ë¡¤ëŸ¬ ì‹¤í–‰"""
    crawler = HeraldCrawler()
    await _run_crawler(crawler, "í—¤ëŸ´ë“œê²½ì œ")


async def run_yahoo_crawler():
    """Yahoo Finance í¬ë¡¤ëŸ¬ ì‹¤í–‰"""
    crawler = YahooCrawler()
    await _run_crawler(crawler, "Yahoo Finance")


async def run_reuters_crawler():
    """Reuters í¬ë¡¤ëŸ¬ ì‹¤í–‰"""
    crawler = ReutersCrawler()
    await _run_crawler(crawler, "Reuters")


async def _run_crawler(crawler, source_name: str):
    """ê³µí†µ í¬ë¡¤ëŸ¬ ì‹¤í–‰ ë¡œì§"""
    news_service = NewsService()
    
    try:
        log.info(f"{source_name} ë‰´ìŠ¤ í¬ë¡¤ë§ ì‹œì‘ (ìµœëŒ€ {MAX_NEWS}ê°œ)")
        news_list = await crawler.crawl(max_news=MAX_NEWS)
        
        if not news_list:
            log.warning(f"{source_name}: í¬ë¡¤ë§ëœ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        success_count = 0
        duplicate_count = 0
        error_count = 0
        
        for news in news_list:
            try:
                if await news_service.check_url_exists(news.url):
                    log.info(f"{source_name}: ì´ë¯¸ ì¡´ì¬í•˜ëŠ” ë‰´ìŠ¤ (ìŠ¤í‚µ)")
                    duplicate_count += 1
                    continue
                
                news_dto = news.to_create_dto()
                result = await news_service.create_news(news_dto)
                
                if result:
                    success_count += 1
                    log.info(f"{source_name}: ë‰´ìŠ¤ ì €ì¥ ì„±ê³µ [{success_count}]")
                else:
                    error_count += 1
                    
            except Exception as e:
                error_count += 1
                log.error(f"{source_name}: ë‰´ìŠ¤ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                continue
        
        log.info(f"""
        âœ… {source_name} ë‰´ìŠ¤ í¬ë¡¤ë§ ì™„ë£Œ
        ğŸ“Š í¬ë¡¤ë§: {len(news_list)}ê°œ
        âœ¨ ì €ì¥ ì„±ê³µ: {success_count}ê°œ
        ğŸ”„ ì¤‘ë³µ ìŠ¤í‚µ: {duplicate_count}ê°œ
        âŒ ì˜¤ë¥˜: {error_count}ê°œ
        """)
        
    except Exception as e:
        log.error(f"{source_name}: í¬ë¡¤ëŸ¬ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    finally:
        await news_service.close()


async def run_all_crawlers():
    """ëª¨ë“  í¬ë¡¤ëŸ¬ ì‹¤í–‰"""
    log.info("=== ì „ì²´ í¬ë¡¤ëŸ¬ ì‹¤í–‰ ì‹œì‘ ===")
    
    # 1. ë„¤ì´ë²„ ê¸ˆìœµ
    await run_naver_crawler()
    
    # 2. í•œêµ­ê²½ì œ
    await run_hankyung_crawler()
    
    # 3. ë§¤ì¼ê²½ì œ
    await run_mk_crawler()
    
    # 4. ì´ë°ì¼ë¦¬
    await run_edaily_crawler()
    
    # 5. í—¤ëŸ´ë“œê²½ì œ
    await run_herald_crawler()
    
    # 6. Yahoo Finance
    await run_yahoo_crawler()
    
    # 7. Reuters
    await run_reuters_crawler()
    
    log.info("=== ì „ì²´ í¬ë¡¤ëŸ¬ ì‹¤í–‰ ì™„ë£Œ ===")


if __name__ == "__main__":
    import uvicorn
    
    # ê°œë°œ ì„œë²„ ì‹¤í–‰
    uvicorn.run(
        "app.main:app",
        host="0.0.0.0",
        port=8000,
        reload=True,  # ì½”ë“œ ë³€ê²½ ì‹œ ìë™ ì¬ì‹œì‘
        log_level="info"
    )